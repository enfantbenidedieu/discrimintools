\Chapter{\bf Analyse Discriminante Linéaire}

Ce chapitre a pour objectif de présenter rapidement les principales fonctionnalités offertes par le package \og discrimintools \fg{} pour réaliser une Analyse Discriminante Linéaire. 

## Présentation des données

L'analyse discriminante linéaire fait partie des technique d'analyse discriminante prédictive. C'est une méthode prédictive où le modèle s'exprime sous la forme d'un système d'équations linéaires des variables explicatives. Il s'agit d'expliquer et de prédire l'appartenance d'un individu à une classe (groupe) prédéfinie à partir de ses caractéristiques mesurées à l'aide de variables prédictives.

### Importation des données

Nous utilisons les données \og alcool \fg{}(\emph{cf.} \href{https://eric.univ-lyon2.fr/ricco/tanagra/fichiers/fr_Tanagra_LDA_Python.pdf}{fr\_Tanagra\_LDA\_Python.pdf}). Il s'agit de prédire le TYPE d'alcool (KIRSCH, MIRAB, POIRE) à partir de ses composants (butanol, méthanol, etc; $8$ variables).

```{python}
# Chargement des données
import pandas as pd
DTrain = pd.read_excel("./data/Eau_de_vie_LDA.xlsx",sheet_name="TRAIN")
print(DTrain.info())
```

### Distribution relative

Nous calculons la distribution relative des classes :

```{python}
# Distribution relative des classes
d = (DTrain.TYPE.value_counts(normalize=True).to_frame()
                .rename(columns={"index":"TYPE"}))
print(d)
```

Les classes semblent assez équilibrées.

## LDA

### Modélisation avec discrimintools

Sage précaution avec les packages pour Python, nous affichons le numéro de la version de \og discrimintools \fg{} utilisée dans ce tutoriel. 

```{python}
# version
import discrimintools
print(discrimintools.__version__)
```

Nous fonctionnons avec la version \og 0.0.1 \fg{}.

```{python}
# Importation
from discrimintools import LDA
```

On crée une instance de la classe LDA, en lui passant ici des étiquettes pour les lignes et les variables. 

```{python}
# Instanciation
lda = LDA(target=["TYPE"],priors = "prop")
```

On estime le modèle en appliquant la méthode \texttt{.fit} de la classe LDA sur le jeu de données.

```{python}
# Entraînement du modèle
lda.fit(DTrain)
```

L'exécution de la méthode \texttt{lda.fit(D)} provoque le calcul de plusieurs attributs parmi lesquels \texttt{lda.coef\_}. Ce champ nous intéresse particulièrement car il correspond aux coefficients des fonctions de classement.

```{python}
# Coefficients des fonctions de score
print(lda.coef_)
```

Le tableau est de dimension $(8,3)$ puisque nous avons un problème à $(K=3)$ classes (le nombre de modalités de la variable cible origine) et $8$ descripteurs.

Il ne faut pas oublier les constantes (intercept) des fonctions linéaires :

```{python}
# et les constantes pour chaque classe
print(lda.intercept_)
```


```{python}
# Summary
from discrimintools import summaryLDA
summaryLDA(lda)
```


## Inspection de l'objet LDA

\begin{itemize}
\item \texttt{call\_["priors"]} correspond à la distribution relative des classes.
\end{itemize}

```{python}
# distribution des classes
priors = lda.call_["priors"]
print(priors)
```

\begin{itemize}
\item \texttt{summary\_information\_} correspond à la distribution absolue et relative des classes 
\end{itemize}

```{python}
# distribution absolue et relative des classes
print(lda.summary_information_)
```


\begin{itemize}
\item \texttt{statistics\_["Eta2"]} correspond au rapport de corrélation $\eta^{2}(X,y)$ entre les variables explicatives et la variable expliquée.
\end{itemize}

```{python}
# Rapport de corrélation
print(lda.statistics_["Eta2"])
```

\begin{itemize}
\item \texttt{classes\_["mean"]} indique les moyennes des variables conditionnellement aux classes
\end{itemize}

```{python}
# moyennes conditionnelles des variables
print(lda.classes_["mean"])
```


\begin{itemize}
\item \texttt{classes\_["mahalanobis"]} indique la matrice des distances (au carré) de Mahalanobis
\end{itemize}

```{python}
# Matrice des distances (au carré) de Mahalanobis
print(lda.classes_["mahalanobis"])
```

## Evaluation globale du modèle

### Statistiques multivariées

Le test de significativité globale du modèle est basé sur l'écartement entre les barycentres conditionnels pour l'analyse discriminante.

```{python}
# MANOVA Test
print(lda.statistics_["manova"])
```

Nous nous intéressons en particulier à la ligne relative à \og Wilks' Lambda \fg{}.

### Matrice de covariance

#### Matrice de covariance intra - classe

Elle est directement fournie par l'objet \og discrimintools \fg{}.

```{python}
# Matrice de covariance intra - classe
print(lda.cov_["within"])
```

#### Matrice de covariance totale

La matrice de covariance totale est proposée par l'objet \og discrimintools \fg{}.

```{python}
# Matrice de covariance totale
print(lda.cov_["total"])
```

#### Matrice de covariance inter - classe

La matrice de covariance inter - classe est proposée par l'objet \og discrimintools \fg{}.

```{python}
# Matrice de covariance inter - classe
print(lda.cov_["between"])
```

### Autres indicateurs : Lambda de Wilks, Transformation de RAO et de Bartlett.

Ces trois indicateurs sont retournés par l'objet \og discrimintools \fg{}.

```{python}
# MANOVA test
global_perf = lda.statistics_["performance"]
print(global_perf)
```

## Evaluation des contributions des variables

Mesurer l'impact des variables est crucial pour l'interprétation du mécanisement d'affectation. Pour l'analyse discriminante, il est possible de produire une mesure d'importance des variables basée sur leurs contributions à la discrimination. Concrètement, il s'agit simplement d'opposer les lambdas de Wilks avec ou sans la variable à évaluer.

### Affichage des contributions sous Python

Ces résultats sont fournis directement par l'objet \og discrimintools \fg{}

```{python}
# Evaluation statistique
stats_eval = lda.statistics_["statistical_evaluation"]
print(stats_eval)
```

## Evaluation en Test

L'évaluation sur l'échantillon test est une approche priviligiée pour mesurer et comparer les performances des modèles de nature et de complexité différente. Dans cette section, nous traitons la seconde feuille \og TEST \fg{} comportant $50$ observations de notre classeur Excel.

### Importation des données

Nous chargeons la feuille \og TEST \fg{}. 

```{python}
# chargement échantillon test
DTest = pd.read_excel("./data/Eau_de_vie_LDA.xlsx",sheet_name="TEST")
print(DTest.info())
```

Nous affichons pour vérification la distribution des classes.

```{python}
# Distribution relative des classes
dtest = (DTest.TYPE.value_counts(normalize=True).reset_index()
                   .rename(columns={"index":"TYPE"}))
print(dtest)
```

Elle est similaire à celle de l'échantillon \og TRAIN \fg{}.

### Prédiction des classes sur l'échantillon d'apprentissage

Il y a deux étapes dans l'évaluation : 
\begin{enumerate}
\item Effectuer la prédiction à partir de la matrice des explicatives de l'échantillon test;
\item Confronter les prédictions de l'étape 1 avec les classes observées.
\end{enumerate}

### Probabilité d'appartenance

L'objet \og discrimintools \fg{} calcule les probabilités d'affectation aux classes avec \texttt{predict\_proba()}. Elle permettent une analyse plus fine de la qualité du modèle, via la construction de la courbe ROC par exemple, dont le principe reste valable pour les problèmes multi - classes.

```{python}
# Matrice X en Test
XTest = DTest[DTest.columns[1:]]
# Probabilité d'appartenance
print(lda.predict_proba(XTest).head(6))
```


### Classe d'appartenance

L'objet \og discrimintools \fg{} calcule les classes d'appartenance avec la fonction \texttt{predict()}. Elle permet de produire les prédictions à partir de la matrice des explicatives en test.

```{python}
# Prédiction sur XTest
y_pred = lda.predict(XTest)
```

On calcule la distribution d'appartenance

```{python}
# Distribution des classes prédictes
y_pred.value_counts(normalize=False).to_frame()
```

$19$ observations ont été prédite \og MIRAB \fg{}, $16$ \og POIRE \fg{} et $15$ \og KIRSCH \fg{}.

### Matrice de confusion et taux de bon classement

La matrice de confusion est issue de la confrontation entre ces prédictions et les classes observées. Nous faisons appel au module \og \href{https://scikit-learn.org/stable/modules/model_evaluation.html}{metrics} \fg{} de la librairie \og \href{https://scikit-learn.org/stable/index.html}{scikit-learn} \fg{}.

```{python,fig.cap = "Matrice de confusion",out.width="70%"}
# Matrice de confusion
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(DTest.TYPE,y_pred,labels=lda.classes_["classes"])
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=lda.classes_["classes"])
disp.plot(cmap=plt.cm.Blues,values_format='g');
plt.show()
```


La fonction \texttt{score()} nous donne le taux de reconnaissance (ou taux de succès).

```{python}
# Taux de succès
print(lda.score(XTest,DTest.TYPE))
```

Notre taux de succès est de $82\%$.

La fonction \texttt{classification\_report()} génère un rapport sur les performances globales, mais aussi sur les reconnaissances par classe (rappel, précision et F-Measure[F1-Score])

```{python}
# rapport
from sklearn.metrics import classification_report
print(classification_report(DTest.TYPE,y_pred))
```

Nous retrouvons, entre autres le taux de succès de $82\%$.

## Sélection de variables

Limiter le modèle aux variables explicatives pertinentes est primordial pour l'interprétation et le deploiement des modèles.

### Backward selection

```{python}
# Selection backward
from discrimintools import STEPDISC
backward=STEPDISC(lda,method="backward",alpha=0.01,model_train=True,verbose=True)
```

Les variables sélectionnées sont les suivantes : 

```{python}
# Variables sélectionnées
selectedVar = backward.results_["selected"]
selectedVar
```

De plus, le paramètre \og model\_train \fg{} permet d'entraîner le modèle LDA avec les variables sélectionnées.

```{python}
# Modèle réduit
lda2 = backward.results_["train"]
lda2
```


```{python}
# Summary
summaryLDA(lda2,to_markdown=False)
```


### Forward selection

```{python}
# Selection forward
forward = STEPDISC(lda,method="forward",alpha=0.01,model_train=True,verbose=True)
```

Les variables sélectionnées sont les suivantes : 

```{python}
# Variables sélectionnées
selectedVar2 = forward.results_["selected"]
selectedVar2
```

Nous entraînons le modèle avec les variables sélectionnées :

```{python}
# Modèle réduit
lda3 = backward.results_["train"]
```


```{python}
# Summary
summaryLDA(lda3,to_markdown=False)
```

